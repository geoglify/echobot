services:
    openWebUI:
        image: ghcr.io/open-webui/open-webui:main
        restart: always
        ports:
            - "5000:8080"
        extra_hosts:
            - "host.docker.internal:host-gateway"
        volumes:
            - open-webui-local:/app/backend/data

    ollama:
        image: ollama/ollama
        container_name: ollama
        ports:
            - "11434:11434"
        devices:
            - nvidia.com/gpu=all
        command:
            - bash
            - -c
            - |
              nvidia-smi -L
        volumes:
            - ollama-local:/root/.ollama
            - ./ollama/entrypoint.sh:/entrypoint.sh
        entrypoint: [ "/usr/bin/bash", "/entrypoint.sh" ]
        
    app:
        build: ./app
        container_name: app
        ports:
            - "3000:3000"
        command: 'node main.js'

volumes:
    ollama-local:
    open-webui-local:
